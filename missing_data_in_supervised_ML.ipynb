{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Missing data in supervised ML</center>\n",
    "### <center>Andras Zsom</center>\n",
    "<center>Lead Data Scientist and  Adjunct Lecturer in Data Science</center>\n",
    "<center>Center for Computation and Visualization</center>\n",
    "<center>Brown University</center>\n",
    "<center>Providence, RI, USA</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About me\n",
    "- Born and raised in Hungary\n",
    "- Astrophysics PhD at MPIA, Heidelberg, Germany\n",
    "- Postdoctoral researcher at MIT (still in astrophysics at the time)\n",
    "- Started at Brown in December 2015 as a Data Scientist\n",
    "- Promoted to Lead Data Scientist in 2017\n",
    "- Adjunct Lecturer in Data Science this semester\n",
    "   - Teaching the course *DATA1030: Hands-on data science* to the DS master students at Brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Science at Brown\n",
    "- Center for Computation and Visualization\n",
    "- Institutional Data group\n",
    "   - Data-driven decision support and predictive modeling for Brownâ€™s administrative units\n",
    "   - Academic research on data-intensive projects\n",
    "- **OPEN POSITION** - more on this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to\n",
    "- Describe the three main types of missingness patterns\n",
    "- Evaluate simple approaches for handling missing values\n",
    "- Apply multivariate imputation\n",
    "- Apply XGBoost to a dataset with missing values\n",
    "- Apply the reduced-features model (also called the pattern submodel approach)\n",
    "- Decide which approach is best for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before we start, a few words on our dataset: kaggle house price\n",
    "- good for educational purposes\n",
    "   - messy data that requires quite a bit of preprocessing\n",
    "   - a nice mixture of continuous, ordinal, and categorical features, each feature type has missing values\n",
    "- lots of excellent kernels on kaggle\n",
    "   - check them out [here]()\n",
    "- dataset and description available in repo\n",
    "   - let's take a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- **Describe the three main types of missingness patterns**\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Missing values often occur in datasets\n",
    "- survey data: not everyone answers all the questions\n",
    "- medical data: not all tests/treatments/etc are performed on all patients\n",
    "- sensor can be offline or malfunctioning\n",
    "- customer data: not every user uses all features of an app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Missing values are an issue for multiple reasons\n",
    "\n",
    "#### Concenptual reason\n",
    "- missing values can introduce biases\n",
    "    - bias: the samples (the data points) are not representative of the underlying distribution/population\n",
    "    - any conclusion drawn from a biased dataset is also biased.\n",
    "    - rich people tend to not fill out survey questions about their salaries and the mean salary estimated from survey data tend to be lower than true value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Practical reason\n",
    "- missing values (NaN, NA, inf) are incompatible with sklearn\n",
    "   - all values in an array need to be numerical otherwise sklearn will throw a *ValueError*\n",
    "- there are a few supervised ML techniques that work with missing values (e.g., XGBoost, CatBoost)\n",
    "   - we will cover those later today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Missing data patterns\n",
    "\n",
    "- **MCAR** - Missing Complete At Random\n",
    "   - some people skip some survey questions by accident\n",
    "- **MAR** - Missing At Random\n",
    "   - males are less likely to fill out a survey on depression\n",
    "   - this has nothing to do with their level of depression after accounting for maleness\n",
    "- **MNAR** - Missing Not At Random\n",
    "   - depressed people are less likely to fill out a survey on depression due to their level of depression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MCAR test\n",
    "\n",
    "- MCAR can be diagnosed with a statistical test ([Little, 1988](https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478722))\n",
    "   - python implementation available in the [pymice](https://github.com/RianneSchouten/pymice) package or in the skipped slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# from the pymice package \n",
    "# https://github.com/RianneSchouten/pymice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as ma\n",
    "import scipy.stats as st\n",
    "\n",
    "def checks_input_mcar_tests(data):\n",
    "    \"\"\" Checks whether the input parameter of class McarTests is correct\n",
    "            Parameters\n",
    "            ----------\n",
    "            data:\n",
    "                The input of McarTests specified as 'data'\n",
    "            Returns\n",
    "            -------\n",
    "            bool\n",
    "                True if input is correct\n",
    "            \"\"\"\n",
    "\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        print(\"Error: Data should be a Pandas DataFrame\")\n",
    "        return False\n",
    "\n",
    "    if not any(data.dtypes.values == np.float):\n",
    "        if not any(data.dtypes.values == np.int):\n",
    "            print(\"Error: Dataset cannot contain other value types than floats and/or integers\")\n",
    "            return False\n",
    "\n",
    "    if not data.isnull().values.any():\n",
    "        print(\"Error: No NaN's in given data\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def mcar_test(data):\n",
    "    \"\"\" Implementation of Little's MCAR test\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Pandas DataFrame\n",
    "        An incomplete dataset with samples as index and variables as columns\n",
    "    Returns\n",
    "    -------\n",
    "    p_value: Float\n",
    "        This value is the outcome of a chi-square statistical test, testing whether the null hypothesis\n",
    "        'the missingness mechanism of the incomplete dataset is MCAR' can be rejected.\n",
    "    \"\"\"\n",
    "\n",
    "    if not checks_input_mcar_tests(data):\n",
    "        raise Exception(\"Input not correct\")\n",
    "\n",
    "    dataset = data.copy()\n",
    "    vars = dataset.dtypes.index.values\n",
    "    n_var = dataset.shape[1]\n",
    "\n",
    "    # mean and covariance estimates\n",
    "    # ideally, this is done with a maximum likelihood estimator\n",
    "    gmean = dataset.mean()\n",
    "    gcov = dataset.cov()\n",
    "\n",
    "    # set up missing data patterns\n",
    "    r = 1 * dataset.isnull()\n",
    "    mdp = np.dot(r, list(map(lambda x: ma.pow(2, x), range(n_var))))\n",
    "    sorted_mdp = sorted(np.unique(mdp))\n",
    "    n_pat = len(sorted_mdp)\n",
    "    correct_mdp = list(map(lambda x: sorted_mdp.index(x), mdp))\n",
    "    dataset['mdp'] = pd.Series(correct_mdp, index=dataset.index)\n",
    "\n",
    "    # calculate statistic and df\n",
    "    pj = 0\n",
    "    d2 = 0\n",
    "    for i in range(n_pat):\n",
    "        dataset_temp = dataset.loc[dataset['mdp'] == i, vars]\n",
    "        select_vars = ~dataset_temp.isnull().any()\n",
    "        pj += np.sum(select_vars)\n",
    "        select_vars = vars[select_vars]\n",
    "        means = dataset_temp[select_vars].mean() - gmean[select_vars]\n",
    "        select_cov = gcov.loc[select_vars, select_vars]\n",
    "        mj = len(dataset_temp)\n",
    "        parta = np.dot(means.T, np.linalg.solve(select_cov, np.identity(select_cov.shape[1])))\n",
    "        d2 += mj * (np.dot(parta, means))\n",
    "\n",
    "    df = pj - n_var\n",
    "\n",
    "    # perform test and save output\n",
    "    p_value = 1 - st.chi2.cdf(d2, df)\n",
    "\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Takeaway\n",
    "- it can be challenging to infer the missingness pattern from an incomplete dataset\n",
    "   - There is a statistical test to diagnose MCAR\n",
    "   - MAR and MNAR are difficult/impossible to diagnose to the best of my knowledge\n",
    "- multiple patterns can be present in the data\n",
    "   - even worse, multiple patterns can be present in one feature!\n",
    "   - missing values in a feature can occur due to a mix of MCAR, MAR, MNAR \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- **Evaluate simple approaches for handling missing values**\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple approaches for handling missing values\n",
    "- 1) categorical/ordinal features: treat missing values as another category\n",
    "    - missing values in categorical/ordinal features are not a big deal\n",
    "- 2) continuous features: this is the tough part\n",
    "    - sklearn's SimpleImputer\n",
    "- 3) exclude points or features with missing values\n",
    "    - might be OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1a) Missing values in a categorical feature\n",
    "- YAY - this is not an issue at all!\n",
    "- Categorical feature needs to be one-hot encoded anyway\n",
    "- Just replace the missing values with 'NA' or 'missing' and treat it as a separate category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1b) Missing values in a ordinal feature\n",
    "- this can be a bit trickier but usually fine\n",
    "- Ordinal encoder is applied to ordinal features\n",
    "    - where does 'NA' or 'missing' fit into the order of the categories?\n",
    "    - usually first or last\n",
    "- if you can figure this out, you are golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 79)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's load the data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# drop the ID\n",
    "df.drop(columns=['Id'],inplace=True)\n",
    "\n",
    "# the target variable\n",
    "y = df['SalePrice']\n",
    "df.drop(columns=['SalePrice'],inplace=True)\n",
    "# the unprocessed feature matrix\n",
    "X = df.values\n",
    "print(X.shape)\n",
    "# the feature names\n",
    "ftrs = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour  \\\n",
      "0         20       RL          90   11694   Pave   NaN      Reg         Lvl   \n",
      "1         20       RL          60    6600   Pave   NaN      Reg         Lvl   \n",
      "2         30       RL          80   13360   Pave  Grvl      IR1         HLS   \n",
      "3         20       RL         NaN   13265   Pave   NaN      IR1         Lvl   \n",
      "4         20       RL         118   13704   Pave   NaN      IR1         Lvl   \n",
      "\n",
      "  Utilities LotConfig  ... ScreenPorch PoolArea PoolQC Fence MiscFeature  \\\n",
      "0    AllPub    Inside  ...         260        0    NaN   NaN         NaN   \n",
      "1    AllPub    Inside  ...           0        0    NaN   NaN         NaN   \n",
      "2    AllPub    Inside  ...           0        0    NaN   NaN         NaN   \n",
      "3    AllPub   CulDSac  ...           0        0    NaN   NaN         NaN   \n",
      "4    AllPub    Corner  ...           0        0    NaN   NaN         NaN   \n",
      "\n",
      "  MiscVal MoSold YrSold SaleType SaleCondition  \n",
      "0       0      7   2007      New       Partial  \n",
      "1       0      8   2009       WD        Normal  \n",
      "2       0      8   2009       WD        Normal  \n",
      "3       0      7   2008       WD        Normal  \n",
      "4       0      1   2006       WD        Normal  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "# let's split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train = pd.DataFrame(data=X_train,columns=ftrs)\n",
    "X_test = pd.DataFrame(data=X_test,columns=ftrs)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# collect the various features\n",
    "cat_ftrs = ['MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood','Condition1','Condition2',\\\n",
    "            'BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation',\\\n",
    "           'Heating','CentralAir','Electrical','GarageType','PavedDrive','MiscFeature','SaleType','SaleCondition']\n",
    "ordinal_ftrs = ['LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\\\n",
    "               'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish',\\\n",
    "               'GarageQual','GarageCond','PoolQC','Fence']\n",
    "ordinal_cats = [['Reg','IR1','IR2','IR3'],['AllPub','NoSewr','NoSeWa','ELO'],['Gtl','Mod','Sev'],\\\n",
    "               ['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Po','Fa','TA','Gd','Ex'],['NA','No','Mn','Av','Gd'],['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\\\n",
    "               ['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Unf','RFn','Fin'],['NA','Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\n",
    "               ['NA','Fa','TA','Gd','Ex'],['NA','MnWw','GdWo','MnPrv','GdPrv']]\n",
    "num_ftrs = ['MSSubClass','LotFrontage','LotArea','OverallQual','OverallCond','YearBuilt','YearRemodAdd',\\\n",
    "             'MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF',\\\n",
    "             'LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr',\\\n",
    "             'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF',\\\n",
    "             'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# one-hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "# ordinal encoder\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer2', SimpleImputer(strategy='constant',fill_value='NA')),\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# collect the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('ord', ordinal_transformer, ordinal_ftrs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 226)\n",
      "(292, 226)\n",
      "(1460, 226)\n"
     ]
    }
   ],
   "source": [
    "# fit_transform the data\n",
    "X_prep = preprocessor.fit_transform(X_train)\n",
    "# little hacky, but collect feature names\n",
    "feature_names = preprocessor.transformers_[0][-1] + \\\n",
    "                list(preprocessor.named_transformers_['cat'][1].get_feature_names(cat_ftrs)) + \\\n",
    "                preprocessor.transformers_[2][-1]\n",
    "\n",
    "df_prep = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "df_prep['SalePrice'] = y_train\n",
    "\n",
    "print(df_prep.shape)\n",
    "\n",
    "df_prep.to_csv('data/house_price_prep.csv',index=False)\n",
    "\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "df_test['SalePrice'] = y_test\n",
    "\n",
    "print(df_test.shape)\n",
    "\n",
    "df = pd.concat([df_prep,df_test],axis=0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's take a closer look at our missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value of the mcar test: 0.0\n",
      "data dimensions: (1168, 226)\n",
      "fraction of missing values in features:\n",
      "LotFrontage    0.181507\n",
      "MasVnrArea     0.005137\n",
      "GarageYrBlt    0.049658\n",
      "SalePrice      0.206336\n",
      "dtype: float64\n",
      "fraction of points with missing values: 0.3895547945205479\n"
     ]
    }
   ],
   "source": [
    "print('p value of the mcar test:',mcar_test(df))\n",
    "print('data dimensions:',df_prep.shape)\n",
    "perc_missing_per_ftr = df_prep.isnull().sum(axis=0)/df_prep.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "frac_missing = sum(df_prep.isnull().sum(axis=1)!=0)/df_prep.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2) Continuous features: mean or median imputation\n",
    "- Imputation means you infer the missing values from the known part of the data\n",
    "- sklearn's SimpleImputer can do mean and median imputation\n",
    "- USUALLY A BAD IDEA!\n",
    "   - MCAR: mean/median of non-missing values is the same as the mean/median of the true underlying distribution, but the variances are different\n",
    "   - not MCAR: the mean/median and the variance of the completed dataset will be off\n",
    "   - supervised ML model is too confident (MCAR) or systematically off (not MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3) Exclude points or features with missing values\n",
    "\n",
    "- easy to do with pandas\n",
    "- it is an ACCEPTABLE approach under two conditions:\n",
    "    - Little's test supports MCAR (p > 0.05)\n",
    "    - only small fraction of points contain missing values (maybe a few percent?)\n",
    "    - the missing values are limited to one or a few features and a large fraction of points are missing from those features (maybe up to 90%?)\n",
    "- if the MCAR assumption is justified, dropping points will not introduce biases to your model\n",
    "- due to the smaller sample size, the confidence of your model might suffer. \n",
    "- what will you do with missing values when you deploy the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 226)\n",
      "(713, 226)\n",
      "(1168, 222)\n"
     ]
    }
   ],
   "source": [
    "print(df_prep.shape)\n",
    "# by default, rows/points are dropped\n",
    "df_r = df_prep.dropna()\n",
    "print(df_r.shape)\n",
    "# drop features with missing values\n",
    "df_c = df_prep.dropna(axis=1)\n",
    "print(df_c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- **Apply multivariate imputation**\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## sklearn's IterativeImputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- **Apply XGBoost to a dataset with missing values**\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost and missing values\n",
    "- sklearn assumes the feature matrix (X) and the target variable (y) are complete \n",
    "- XGBoost doesn't! \n",
    "   - it runs just fine with np.nans in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OneHotEncoder in module sklearn.preprocessing._encoders:\n",
      "\n",
      "class OneHotEncoder(_BaseEncoder)\n",
      " |  Encode categorical integer features as a one-hot numeric array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
      " |  encoding scheme. This creates a binary column for each category and\n",
      " |  returns a sparse matrix or dense array.\n",
      " |  \n",
      " |  By default, the encoder derives the categories based on the unique values\n",
      " |  in each feature. Alternatively, you can also specify the `categories`\n",
      " |  manually.\n",
      " |  The OneHotEncoder previously assumed that the input features take on\n",
      " |  values in the range [0, max(values)). This behaviour is deprecated.\n",
      " |  \n",
      " |  This encoding is needed for feeding categorical data to many scikit-learn\n",
      " |  estimators, notably linear models and SVMs with the standard kernels.\n",
      " |  \n",
      " |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
      " |  instead.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of lists/arrays of values, default='auto'.\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values within a single feature, and should be sorted in case of\n",
      " |        numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |  drop : 'first' or a list/array of shape (n_features,), default=None.\n",
      " |      Specifies a methodology to use to drop one of the categories per\n",
      " |      feature. This is useful in situations where perfectly collinear\n",
      " |      features cause problems, such as when feeding the resulting data\n",
      " |      into a neural network or an unregularized regression.\n",
      " |  \n",
      " |      - None : retain all features (the default).\n",
      " |      - 'first' : drop the first category in each feature. If only one\n",
      " |        category is present, the feature will be dropped entirely.\n",
      " |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
      " |        should be dropped.\n",
      " |  \n",
      " |  sparse : boolean, default=True\n",
      " |      Will return sparse matrix if set True else will return an array.\n",
      " |  \n",
      " |  dtype : number type, default=np.float\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  handle_unknown : 'error' or 'ignore', default='error'.\n",
      " |      Whether to raise an error or ignore if an unknown categorical feature\n",
      " |      is present during transform (default is to raise). When this parameter\n",
      " |      is set to 'ignore' and an unknown category is encountered during\n",
      " |      transform, the resulting one-hot encoded columns for this feature\n",
      " |      will be all zeros. In the inverse transform, an unknown category\n",
      " |      will be denoted as None.\n",
      " |  \n",
      " |  n_values : 'auto', int or array of ints, default='auto'\n",
      " |      Number of values per feature.\n",
      " |  \n",
      " |      - 'auto' : determine value range from training data.\n",
      " |      - int : number of categorical values per feature.\n",
      " |              Each feature value should be in ``range(n_values)``\n",
      " |      - array : ``n_values[i]`` is the number of categorical values in\n",
      " |                ``X[:, i]``. Each feature value should be\n",
      " |                in ``range(n_values[i])``\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The `n_values` keyword was deprecated in version 0.20 and will\n",
      " |          be removed in 0.22. Use `categories` instead.\n",
      " |  \n",
      " |  categorical_features : 'all' or array of indices or mask, default='all'\n",
      " |      Specify what features are treated as categorical.\n",
      " |  \n",
      " |      - 'all': All features are treated as categorical.\n",
      " |      - array of indices: Array of categorical feature indices.\n",
      " |      - mask: Array of length n_features and with dtype=bool.\n",
      " |  \n",
      " |      Non-categorical features are always stacked to the right of the matrix.\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The `categorical_features` keyword was deprecated in version\n",
      " |          0.20 and will be removed in 0.22.\n",
      " |          You can use the ``ColumnTransformer`` instead.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during fitting\n",
      " |      (in order of the features in X and corresponding with the output\n",
      " |      of ``transform``). This includes the category specified in ``drop``\n",
      " |      (if any).\n",
      " |  \n",
      " |  drop_idx_ : array of shape (n_features,)\n",
      " |      ``drop_idx_[i]`` isÂ the index in ``categories_[i]`` of the category to\n",
      " |      be dropped for each feature. None if all the transformed features will\n",
      " |      be retained.\n",
      " |  \n",
      " |  active_features_ : array\n",
      " |      Indices for active features, meaning values that actually occur\n",
      " |      in the training set. Only available when n_values is ``'auto'``.\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The ``active_features_`` attribute was deprecated in version\n",
      " |          0.20 and will be removed in 0.22.\n",
      " |  \n",
      " |  feature_indices_ : array of shape (n_features,)\n",
      " |      Indices to feature ranges.\n",
      " |      Feature ``i`` in the original data is mapped to features\n",
      " |      from ``feature_indices_[i]`` to ``feature_indices_[i+1]``\n",
      " |      (and then potentially masked by ``active_features_`` afterwards)\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The ``feature_indices_`` attribute was deprecated in version\n",
      " |          0.20 and will be removed in 0.22.\n",
      " |  \n",
      " |  n_values_ : array of shape (n_features,)\n",
      " |      Maximum number of values per feature.\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The ``n_values_`` attribute was deprecated in version\n",
      " |          0.20 and will be removed in 0.22.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to a binary one-hot encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OneHotEncoder\n",
      " |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  ... # doctest: +ELLIPSIS\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
      " |     dtype=<... 'numpy.float64'>, handle_unknown='ignore',\n",
      " |     n_values=None, sparse=True)\n",
      " |  \n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
      " |  array([[1., 0., 1., 0., 0.],\n",
      " |         [0., 1., 0., 0., 0.]])\n",
      " |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
      " |  array([['Male', 1],\n",
      " |         [None, 2]], dtype=object)\n",
      " |  >>> enc.get_feature_names()\n",
      " |  array(['x0_Female', 'x0_Male', 'x1_1', 'x1_2', 'x1_3'], dtype=object)\n",
      " |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
      " |  >>> drop_enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 0., 0.],\n",
      " |         [1., 1., 0.]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OrdinalEncoder : performs an ordinal (integer)\n",
      " |    encoding of the categorical features.\n",
      " |  sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
      " |    dictionary items (also handles string-valued features).\n",
      " |  sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
      " |    encoding of dictionary items or strings.\n",
      " |  sklearn.preprocessing.LabelBinarizer : binarizes labels in a one-vs-all\n",
      " |    fashion.\n",
      " |  sklearn.preprocessing.MultiLabelBinarizer : transforms between iterable of\n",
      " |    iterables and a multilabel format, e.g. a (samples x classes) binary\n",
      " |    matrix indicating the presence of a class label.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OneHotEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_values=None, categorical_features=None, categories=None, drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X, then transform X.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X) but more convenient.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : sparse matrix if sparse=True else a 2-d array\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  get_feature_names(self, input_features=None)\n",
      " |      Return feature names for output features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : list of string, length n_features, optional\n",
      " |          String names for input features if available. By default,\n",
      " |          \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      output_feature_names : array of string, length n_output_features\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the back data to the original representation.\n",
      " |      \n",
      " |      In case unknown categories are encountered (all zeros in the\n",
      " |      one-hot encoding), ``None`` is used to represent this category.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X using one-hot encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : sparse matrix if sparse=True else a 2-d array\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  active_features_\n",
      " |  \n",
      " |  feature_indices_\n",
      " |  \n",
      " |  n_values_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- **Apply the reduced-features model (also called the pattern submodel approach)**\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- **Decide which approach is best for your dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you can\n",
    "- Describe the three main types of missingness patterns\n",
    "- Evaluate simple approaches for handling missing values\n",
    "- Apply multivariate imputation\n",
    "- Apply XGBoost to a dataset with missing values\n",
    "- Apply the reduced-features model (also called the pattern submodel approach)\n",
    "- Decide which approach is best for your dataset\n",
    "\n",
    "### Thanks for you attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
